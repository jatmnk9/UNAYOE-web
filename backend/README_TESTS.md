# ğŸ§ª DocumentaciÃ³n de Pruebas - UNAYOE Backend

## ğŸ“‹ Tabla de Contenidos

1. [IntroducciÃ³n](#introducciÃ³n)
2. [Arquitectura de Pruebas](#arquitectura-de-pruebas)
3. [ConfiguraciÃ³n](#configuraciÃ³n)
4. [EjecuciÃ³n de Pruebas](#ejecuciÃ³n-de-pruebas)
5. [Cobertura de Pruebas](#cobertura-de-pruebas)
6. [MÃ³dulos Probados](#mÃ³dulos-probados)
7. [Mocks y Fixtures](#mocks-y-fixtures)
8. [CI/CD](#cicd)
9. [Mejores PrÃ¡cticas](#mejores-prÃ¡cticas)

---

## ğŸ¯ IntroducciÃ³n

Este proyecto implementa un sistema completo de pruebas automatizadas para el backend de UNAYOE, incluyendo:

- âœ… **Pruebas Unitarias** - Servicios NLP/IA y anÃ¡lisis
- âœ… **Pruebas de IntegraciÃ³n** - MÃ³dulos de Notas, AnÃ¡lisis y Recomendaciones
- âœ… **Mocks** - Supabase y servicios externos
- âœ… **CI/CD** - GitHub Actions automatizado

### ğŸ” MÃ³dulos con Cobertura de Pruebas

**MÃ³dulo 1: Notes + Analysis (con servicios NLP/IA)**
- Router de notas ([app/routers/notes.py](app/routers/notes.py))
- Router de anÃ¡lisis ([app/routers/analysis.py](app/routers/analysis.py))
- Servicio NLP ([app/services/nlp_service.py](app/services/nlp_service.py))
- Servicio de anÃ¡lisis ([app/services/analysis_service.py](app/services/analysis_service.py))

**MÃ³dulo 2: Recommendations (con personalizaciÃ³n NLP)**
- Router de recomendaciones ([app/routers/recommendations.py](app/routers/recommendations.py))
- Servicio de recomendaciones ([app/services/recommendations_service.py](app/services/recommendations_service.py))

---

## ğŸ—ï¸ Arquitectura de Pruebas

```
backend/tests/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ conftest.py                          # ConfiguraciÃ³n global y fixtures
â”œâ”€â”€ pytest.ini                           # ConfiguraciÃ³n de pytest
â”‚
â”œâ”€â”€ unit/                                # Pruebas unitarias
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_nlp_service.py             # Pruebas de servicios NLP/IA
â”‚   â””â”€â”€ test_analysis_service.py        # Pruebas de anÃ¡lisis
â”‚
â”œâ”€â”€ integration/                         # Pruebas de integraciÃ³n
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_notes_analysis_integration.py      # IntegraciÃ³n Notes + Analysis
â”‚   â””â”€â”€ test_recommendations_integration.py     # IntegraciÃ³n Recommendations
â”‚
â”œâ”€â”€ mocks/                               # Mocks para servicios externos
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ supabase_mock.py                # Mock de Supabase
â”‚   â””â”€â”€ nlp_mock.py                     # Mock de servicios NLP/IA
â”‚
â””â”€â”€ fixtures/                            # Fixtures compartidos
    â””â”€â”€ __init__.py
```

---

## âš™ï¸ ConfiguraciÃ³n

### 1. Instalar Dependencias

```bash
cd backend
pip install -r requirements.txt
```

### 2. Configurar Variables de Entorno

Crear archivo `.env` en `backend/`:

```env
APP_NAME="UNAYOE Test"
DEBUG=true
API_VERSION="2.0.0-test"
SUPABASE_URL="https://test.supabase.co"
SUPABASE_KEY="test_key"
CORS_ORIGINS="http://localhost:3000"
SENTIMENT_MODEL="pysentimiento/robertuito-sentiment-analysis"
EMOTION_MODEL="pysentimiento/robertuito-emotion-analysis"
GEMINI_API_KEY="test_gemini_key"
GEMINI_MODEL="gemini-2.0-flash"
GMAIL_SENDER="test@example.com"
GMAIL_SMTP_PASSWORD="test_password"
ALERT_FALLBACK_EMAIL="alert@example.com"
```

---

## ğŸš€ EjecuciÃ³n de Pruebas

### Ejecutar Todas las Pruebas

```bash
cd backend
pytest
```

### Ejecutar Pruebas EspecÃ­ficas

**Pruebas Unitarias:**
```bash
pytest tests/unit -v
```

**Pruebas de IntegraciÃ³n:**
```bash
pytest tests/integration -v
```

**Pruebas de Servicios NLP/IA:**
```bash
pytest -m nlp -v
```

**Pruebas con Marcadores:**
```bash
# Solo pruebas unitarias
pytest -m unit -v

# Solo pruebas de integraciÃ³n
pytest -m integration -v

# Excluir pruebas lentas
pytest -m "not slow" -v
```

### Ejecutar un Archivo EspecÃ­fico

```bash
pytest tests/unit/test_nlp_service.py -v
```

### Ejecutar una Prueba EspecÃ­fica

```bash
pytest tests/unit/test_nlp_service.py::TestNLPService::test_analizar_sentimiento_positivo -v
```

---

## ğŸ“Š Cobertura de Pruebas

### Generar Reporte de Cobertura

```bash
pytest --cov=app --cov-report=html --cov-report=term-missing
```

### Ver Reporte HTML

```bash
# En Windows
start htmlcov/index.html

# En Linux/Mac
open htmlcov/index.html
```

### Objetivo de Cobertura

El proyecto estÃ¡ configurado con un mÃ­nimo de **70% de cobertura**.

---

## ğŸ§© MÃ³dulos Probados

### 1ï¸âƒ£ MÃ³dulo: Notes + Analysis (NLP/IA)

#### Funcionalidades Probadas:

**Servicio NLP ([test_nlp_service.py](tests/unit/test_nlp_service.py)):**
- âœ… Preprocesamiento de texto (tokenizaciÃ³n, limpieza)
- âœ… AnÃ¡lisis de sentimientos (POS/NEG/NEU)
- âœ… AnÃ¡lisis de emociones (joy, sadness, anger, fear)
- âœ… Manejo de textos vacÃ­os y casos extremos
- âœ… Clasificadores mock de transformers

**Servicio de AnÃ¡lisis ([test_analysis_service.py](tests/unit/test_analysis_service.py)):**
- âœ… AnÃ¡lisis de mÃºltiples notas
- âœ… CreaciÃ³n de visualizaciones (grÃ¡ficos de sentimientos, emociones)
- âœ… GeneraciÃ³n de nubes de palabras
- âœ… Manejo de DataFrames vacÃ­os
- âœ… Casos extremos (textos largos, caracteres especiales)

**IntegraciÃ³n ([test_notes_analysis_integration.py](tests/integration/test_notes_analysis_integration.py)):**
- âœ… Flujo completo: Guardar nota â†’ Analizar con NLP â†’ Obtener resultados
- âœ… Endpoints de creaciÃ³n de notas
- âœ… Endpoints de anÃ¡lisis de notas
- âœ… ExportaciÃ³n de reportes CSV
- âœ… Sistema de alertas en background
- âœ… ValidaciÃ³n de datos

#### Ejemplo de Uso:

```python
# Crear una nota y analizarla
response = client.post("/notas", json={
    "note": "Me siento muy feliz hoy",
    "user_id": "test_user_123"
})

# Analizar notas del usuario
response = client.get("/analyze/test_user_123")
```

---

### 2ï¸âƒ£ MÃ³dulo: Recommendations (PersonalizaciÃ³n con NLP)

#### Funcionalidades Probadas:

**IntegraciÃ³n ([test_recommendations_integration.py](tests/integration/test_recommendations_integration.py)):**
- âœ… ObtenciÃ³n de todas las recomendaciones
- âœ… Recomendaciones personalizadas basadas en anÃ¡lisis NLP
- âœ… Sistema de likes (agregar, eliminar, obtener)
- âœ… Recomendaciones favoritas
- âœ… Flujo completo: Nota â†’ AnÃ¡lisis NLP â†’ RecomendaciÃ³n personalizada
- âœ… ConsideraciÃ³n de likes previos en personalizaciÃ³n
- âœ… Casos sin historial (recomendaciones generales)

#### Ejemplo de Uso:

```python
# Obtener recomendaciones personalizadas (usa anÃ¡lisis NLP de notas previas)
response = client.get("/recomendaciones/test_user_123")

# Agregar like a una recomendaciÃ³n
response = client.post("/likes/test_user_123/rec_1")

# Obtener favoritos
response = client.get("/recomendaciones/favoritos/test_user_123")
```

---

## ğŸ­ Mocks y Fixtures

### Mock de Supabase ([supabase_mock.py](tests/mocks/supabase_mock.py))

Simula operaciones de base de datos:
- `select()`, `insert()`, `update()`, `delete()`
- `eq()`, `order()`, `limit()`
- `execute()`

```python
# Uso en pruebas
mock_supabase = MockSupabaseClient()
mock_supabase.seed_data("notas", [
    {"id": "1", "note": "Test", "usuario_id": "user_123"}
])
```

### Mock de Servicios NLP ([nlp_mock.py](tests/mocks/nlp_mock.py))

Simula anÃ¡lisis de NLP sin cargar modelos reales:
- AnÃ¡lisis de sentimientos
- AnÃ¡lisis de emociones
- Preprocesamiento de texto

```python
# Uso en pruebas
mock_nlp = MockNLPService()
sentimiento = mock_nlp.analizar_sentimiento("Me siento feliz")
# Retorna: "POS"
```

### Fixtures Globales ([conftest.py](tests/conftest.py))

Fixtures disponibles para todas las pruebas:
- `test_settings` - ConfiguraciÃ³n de prueba
- `client` - Cliente TestClient de FastAPI
- `mock_supabase_client` - Mock de Supabase
- `mock_nlp_service` - Mock de servicios NLP
- `sample_note_data` - Datos de ejemplo
- `sample_recommendations` - Recomendaciones de ejemplo

---

## ğŸ”„ CI/CD

### GitHub Actions

El proyecto incluye un workflow de CI/CD ([.github/workflows/backend-tests.yml](../.github/workflows/backend-tests.yml)) que se ejecuta automÃ¡ticamente en:
- Push a ramas: `main`, `develop`, `refactorbackend`
- Pull Requests a `main` y `develop`

#### Jobs del Pipeline:

**1. Test Job:**
- âœ… Ejecuta pruebas en Python 3.10 y 3.11
- âœ… Pruebas unitarias
- âœ… Pruebas de servicios NLP/IA
- âœ… Pruebas de integraciÃ³n
- âœ… Reporte de cobertura (Codecov)

**2. Code Quality Job:**
- âœ… VerificaciÃ³n de formato con Black
- âœ… VerificaciÃ³n de imports con isort
- âœ… Linting con flake8

#### Ver Estado del Pipeline:

```bash
# Badge en README (agregar)
![Tests](https://github.com/usuario/UNAYOE-web/workflows/Backend%20Tests%20CI%2FCD/badge.svg)
```

---

## âœ¨ Mejores PrÃ¡cticas

### 1. Nombrar Pruebas de Forma Descriptiva

```python
# âœ… BIEN
def test_analizar_sentimiento_positivo_con_texto_feliz():
    pass

# âŒ MAL
def test_1():
    pass
```

### 2. Usar Marcadores

```python
@pytest.mark.integration
@pytest.mark.nlp
def test_flujo_completo_nlp():
    pass
```

### 3. Usar Fixtures para Datos Compartidos

```python
@pytest.fixture
def sample_note():
    return {"note": "Test", "user_id": "123"}

def test_guardar_nota(sample_note):
    # Usar sample_note
    pass
```

### 4. Probar Casos Extremos

```python
def test_analizar_texto_vacio():
    result = nlp_service.analizar_sentimiento("")
    assert result == "NEU"
```

### 5. Usar Parametrize para MÃºltiples Casos

```python
@pytest.mark.parametrize("texto,sentimiento_esperado", [
    ("Me siento feliz", "POS"),
    ("Estoy triste", "NEG"),
    ("DÃ­a normal", "NEU"),
])
def test_sentimientos(texto, sentimiento_esperado):
    result = nlp_service.analizar_sentimiento(texto)
    assert result == sentimiento_esperado
```

### 6. Mantener Pruebas Independientes

Las pruebas deben poder ejecutarse en cualquier orden sin dependencias entre ellas.

### 7. Usar Mocks para Servicios Externos

Siempre mockear:
- Base de datos (Supabase)
- APIs externas (Gemini, Gmail)
- Modelos de ML (transformers)

---

## ğŸ“ˆ EstadÃ­sticas del Proyecto

| MÃ©trica | Valor |
|---------|-------|
| **Pruebas Totales** | 50+ |
| **Pruebas Unitarias** | 25+ |
| **Pruebas de IntegraciÃ³n** | 25+ |
| **Cobertura MÃ­nima** | 70% |
| **MÃ³dulos Probados** | 2 (Notes+Analysis, Recommendations) |
| **Servicios NLP Probados** | âœ… SÃ­ |

---

## ğŸ†˜ SoluciÃ³n de Problemas

### Error: "ModuleNotFoundError"

```bash
# Asegurarse de estar en el directorio correcto
cd backend

# Reinstalar dependencias
pip install -r requirements.txt
```

### Error: "No module named 'app'"

```bash
# Agregar directorio raÃ­z al PYTHONPATH
export PYTHONPATH="${PYTHONPATH}:$(pwd)"

# O ejecutar desde el directorio backend
cd backend && pytest
```

### Pruebas Lentas

```bash
# Excluir pruebas marcadas como "slow"
pytest -m "not slow"
```

### Ver Salida Detallada

```bash
# Modo verbose con output completo
pytest -v -s
```

---

## ğŸ“š Referencias

- [Pytest Documentation](https://docs.pytest.org/)
- [FastAPI Testing](https://fastapi.tiangolo.com/tutorial/testing/)
- [Pytest-cov](https://pytest-cov.readthedocs.io/)
- [GitHub Actions](https://docs.github.com/en/actions)

---

## ğŸ‘¥ Contribuir

Para agregar nuevas pruebas:

1. Crear archivo de prueba en `tests/unit/` o `tests/integration/`
2. Seguir la convenciÃ³n de nombres: `test_*.py`
3. Usar fixtures del [conftest.py](tests/conftest.py)
4. Agregar marcadores apropiados
5. Ejecutar pruebas localmente antes de commit
6. Verificar cobertura

---

## ğŸ“„ Licencia

Este proyecto es parte de UNAYOE - Sistema de AnÃ¡lisis de Bienestar Estudiantil.

---

**Ãšltima actualizaciÃ³n:** 2025-12-05
**VersiÃ³n:** 2.0.0
